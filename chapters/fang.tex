% !TEX root = ../main.tex

\chapter{FANG} % (fold)
\label{cha:fang}

\section{Hyperparameters}

FANG has four hyperparameters which can be either activated or deactivated: temporality, stance, proxmity and attention.
Table~\ref{tab:fang:hyperparameters}lists the AUC scores obtained with FANG trained with 80\% of the data.
We progressively activated the hyperparameters one by one until all were used.
We otain these scores by running FANG on the misinformation dataset explored in chatpter~\ref{cha:Dataset}.

\begin{table}[h]
    \centering
    \caption{FANG AUC scores with some hyperparameters combination.}
    \label{tab:fang:hyperparameters}
    \begin{tabular}{ccccc}
        \toprule
        Attention & Stance & Proximity & Temporality & AUC Score \\
        \midrule
        \checkmark & & & & 0.0 \\
        \checkmark & \checkmark & & & 0.0 \\
        \checkmark & \checkmark & \checkmark & & 0.0 \\
        \checkmark & \checkmark & \checkmark & \checkmark & 0.0 \\
        \midrule
        \checkmark & \checkmark & \checkmark & \checkmark & \textbf{0.0} \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage
\section{FANG Test Run on CPU}

Typically neural network model of this complexity and size are trained on GPU.

For testing, a CPU flag was created: this allows to train FANG on a machine that only has a CPU.
All possible flags were activated (attention, stance, proximity and temporality).
For this test run, only 10 percent of the data was used.
This is because training on CPU is much much slower compared to GPU.

\begin{lstlisting}[language=bash]
python run_graph.py \
    -t fang \
    -m graph_sage \
    -p data/news_graph \
    --percent 10 \
    --epochs=30 \
    --attention \
    --use-stance \
    --use-proximity \
    --temporal \
    --use_cpu
\end{lstlisting}

We obtained good results for a first run on only 10 percent of the data.
The F1 score is 0.58.
This result is similar to DeepWalk with the best parameters at 10 percent of the training data. 
However we expected to see FANG's score value climb much faster than DeepWalk and node2vec once the percentage of training data is higher.

\begin{lstlisting}
    Number of test: 465
    Accuracy score: 0.6129032258064516
    Precision score: 0.6021869620867724
    Recall score: 0.5872625508819539
    F1 score: 0.5827517447657029
    ROC AUC score: 0.647859188903965
    {'accuracy': 0.6129032258064516, 
    'precision': 0.6021869620867724, 
    'recall': 0.5872625508819539, 
    'f1': 0.5827517447657029, 
    'auc': 0.647859188903965, 
    'loss': 0.702542781829834}
\end{lstlisting}

\newpage
\section{Code Structure}
This is a temporary section, just to have notes when we discuss the code.
\newline
FANG has the following flags to use:
\begin{itemize}
    \item the task: fang or news graph.
    \item the model: graphSAGE or GCN. GraphSAGE can only be used with FANG, and GCN only with news graph.
    \item percent: Percentage for the size of the training set. 10, 30, 50, 70, 90. Hardcoded in JSON files.
    \item temporal: whether to use temporality or not. Default to false. 
    \item stance: whether to use stance or not. Default to false.
    \item promixity: whether to use proximity. Default to false.
    \item attention: whether to use attention. Default to false.

\end{itemize}
\vspace{5mm}
In terms of directories and file:
\begin{itemize}
    \item data/: directory for the FANG dataset. Processed and unprocessed.
    \item dataset/: for the NewsGraph class. This is to run news graph instead of FANG.
    \item fang/: everything related to graphSAGE: implementation, building the graph, loading the data in the graph. Also FakeNewsClassifier and StanceClassifier classes.
    \item graph/: GCN and NGCN. The code only uses NGCN.
    \item training/: for the training of FANG.
    \item use\_embed/: for embedding the users in graphSAGE.
    \item run\_graph.py: this is where FANG begins.
\end{itemize}
\vspace{5mm}

code flow:
\begin{itemize}
    \item start in run\_graph.py. We can either run news\_graph or fang.
    \item news\_graph goes into \verb|training/news_graph.py|. The required model is GCN. It build a NGCN as model. It trains the model, validates and tests it.
    \item fang requires the model to be graphSAGE. The program goes to \verb|fang/graphsage/|. It loads the dataset and a lot of parameters. 
    \item It build the FANG graph, create a StanceClassifier and a FakeNewsClassifier. It starts training, testing and validation.
\end{itemize}
