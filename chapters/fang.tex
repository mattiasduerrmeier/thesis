% !TEX root = ../main.tex

\chapter{FANG} % (fold)
\label{cha:fang}

\section{FANG Test Run on CPU}

FANG uses the PyTorch library. 
Typically neural network model of this complexity and size are trained on GPU.

For testing, a CPU flag was created: this allows to train FANG on a machine that only has a CPU.
All possible flags were activated (attention, stance, proximity and temporality).
For this test run, only 10 percent of the data was used.
This is because training on CPU is much much slower compared to GPU.

\begin{lstlisting}[language=bash]
python run_graph.py \
    -t fang \
    -m graph_sage \
    -p data/news_graph \
    --percent 10 \
    --epochs=30 \
    --attention \
    --use-stance \
    --use-proximity \
    --temporal \
    --use_cpu
\end{lstlisting}

We obtained good results for a first run on only 10 percent of the data.
The F1 score is 0.58.
This result is similar to DeepWalk with the best parameters at 10 percent of the training data. 
However we expected to see FANG's score value climb much faster than DeepWalk and node2vec once the percentage of training data is higher.

\begin{lstlisting}
    Number of test: 465
    Accuracy score: 0.6129032258064516
    Precision score: 0.6021869620867724
    Recall score: 0.5872625508819539
    F1 score: 0.5827517447657029
    ROC AUC score: 0.647859188903965
    {'accuracy': 0.6129032258064516, 
    'precision': 0.6021869620867724, 
    'recall': 0.5872625508819539, 
    'f1': 0.5827517447657029, 
    'auc': 0.647859188903965, 
    'loss': 0.702542781829834}
\end{lstlisting}
